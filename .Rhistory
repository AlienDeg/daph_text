html_children %>%
as.character() %>%
str_extract('\\\".*?\\\"') %>%
str_replace_all(c('\"' = ''))) %>%
separate(urls,
into = c("bin", "bin2", "bin3", "year", "bin4", "bin5", "episode"),
sep = "\\/",
remove = FALSE) %>%
mutate(urls = as.character(urls), episode = str_replace_all(episode, "\\-.*", "")) %>%
select(urls, year, episode)
urls
}
# Pull the urls list from the site. Ignore the "Too many values" error, this comes from the call to separate()
full_urls <- map_df(page_nums, get_url_list)
full_urls
# Split the url DF into a list so it can be iteratively called by map_df in the next step
full_urls_split <- full_urls %>%
split(.$episode)
# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words
get_post_content <- function(ep_df) {
# Quick line for debugging purposes
# print(paste("Running for :", ep_df$url))
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
# Traverse down to the paragraph elements within the 'post' class
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
# Get the post title
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
# Rename the post_ps variable as it's automatically called '.' when converted from a list, which is problematic.
names(post_ps) <- "text"
# Filter to only elements which contain a timecode
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)|(Michael:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)|(Moe:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)|(Tim:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = as.numeric(ms(str_extract(text, "\\d{2}:\\d{2}"))),
time_end = lead(time_start, 1),
duration = time_end - time_start
)
post_ps
} else {
NULL
}
}
daph_post_text <- map_df(full_urls_split, get_post_content)
dir.create("output")
saveRDS(daph_post_text, "datafiles/daph_post_text.rds") # Storing as an rds file locally to avoid
daph_post_text
monologue <- daph_post_text %>%
arrange(desc(duration)) %>%
top_n(10, duration) %>%
select(speaker, text, title, duration)
monologue
monologue$text[1]
data("stop_words")
daph_post_words <- daph_post_text %>%
unnest_tokens(output = word, input = text, format = "text") %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "\\d{2}"))
saveRDS(daph_post_words, "datafiles/daph_post_words.rds")
swears <- daph_post_words %>%
filter(str_detect(word, "(shit)|(fuck)|(crap)"))
swears_by_speaker <- swears %>%
group_by(episode, speaker) %>%
filter(speaker!= "Unknown") %>%
summarise(ep_count = n()) %>%
ungroup() %>%
group_by(speaker) %>%
summarise(avg_swears = mean(ep_count, na.rm = TRUE)) %>%
mutate(speaker = fct_reorder(speaker, avg_swears))
swears_by_speaker %>%
ggplot(aes(x = speaker, y = avg_swears)) +
geom_col(fill = "#3FA0D9") +
theme_hc() +
labs(
x = "Speaker",
y = "Avg. swear jar deposits per episode",
title = "Avg. swear count by speaker",
subtitle = "Was there ever any doubt?")
# word count per episode by presenter
words_by_speaker <- daph_post_words %>%
group_by(episode, speaker) %>%
filter(speaker!= "Unknown") %>%
summarise(ep_count = n()) %>%
ungroup()
wbs_overall <- words_by_speaker %>%
group_by(speaker) %>%
summarise(avg_words = mean(ep_count, na.rm = TRUE)) %>%
mutate(speaker = fct_reorder(speaker, avg_words))
wbs_overall %>%
ggplot(aes(x = speaker, y = avg_words)) +
geom_col(fill = "#3FA0D9") +
theme_hc() +
xlab('Speaker') +
ylab('Avg words spoken per episode') +
ggtitle("Chatty Catty")
wbs_be <- words_by_speaker %>%
filter(ep_count!=0) %>%
ggplot(aes(as.numeric(episode), ep_count, colour = speaker)) +
geom_line(na.rm = TRUE) +
geom_point() +
labs(
title = "Words spoken by episode, by speaker",
y = "Words spoken",
x = "Episode number",
colour = "Speaker Name"
)
library(plotly)
ggplotly(wbs_be)
save_csv(daph_post_text, "daph_post_text.csv")
write_csv(daph_post_text, "daph_post_text.csv")
write_csv(daph_post_words, "daph_post_words.csv")
View(daph_post_words)
View(daph_post_text)
# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words
get_post_content <- function(ep_df) {
# Quick line for debugging purposes
# print(paste("Running for :", ep_df$url))
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
# Traverse down to the paragraph elements within the 'post' class
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
# Get the post title
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
# Rename the post_ps variable as it's automatically called '.' when converted from a list, which is problematic.
names(post_ps) <- "text"
# Filter to only elements which contain a timecode
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)|(Michael:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)|(Moe:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)|(Tim:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = as.numeric(ms(str_extract(text, "\\d{2}:\\d{2}"))),
time_end = lead(time_start, 1),
duration = time_end - time_start
)
post_ps
} else {
NULL
}
}
daph_post_text <- map_df(full_urls_split, get_post_content) %>%
arrange(episode, time_start)
saveRDS(daph_post_text, "datafiles/daph_post_text.rds") # Storing as an rds file locally to avoid
daph_post_text
# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words
get_post_content <- function(ep_df) {
# Quick line for debugging purposes
# print(paste("Running for :", ep_df$url))
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
# Traverse down to the paragraph elements within the 'post' class
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
# Get the post title
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
# Rename the post_ps variable as it's automatically called '.' when converted from a list, which is problematic.
names(post_ps) <- "text"
# Filter to only elements which contain a timecode
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)|(Michael:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)|(Moe:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)|(Tim:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = as.numeric(hms(str_extract(text, "\\d{2}:\\d{2}"))),
time_end = lead(time_start, 1),
duration = time_end - time_start
)
post_ps
} else {
NULL
}
}
daph_post_text <- map_df(full_urls_split, get_post_content) %>%
arrange(episode, time_start)
saveRDS(daph_post_text, "datafiles/daph_post_text.rds") # Storing as an rds file locally to avoid
daph_post_text
# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words
get_post_content <- function(ep_df) {
# Quick line for debugging purposes
# print(paste("Running for :", ep_df$url))
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
# Traverse down to the paragraph elements within the 'post' class
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
# Get the post title
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
# Rename the post_ps variable as it's automatically called '.' when converted from a list, which is problematic.
names(post_ps) <- "text"
# Filter to only elements which contain a timecode
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)|(Michael:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)|(Moe:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)|(Tim:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = as.numeric(hms(str_extract(text, "\\d{2}:\\d{2}:\\d{2}"))),
time_end = lead(time_start, 1),
duration = time_end - time_start
)
post_ps
} else {
NULL
}
}
daph_post_text <- map_df(full_urls_split, get_post_content) %>%
arrange(episode, time_start)
saveRDS(daph_post_text, "datafiles/daph_post_text.rds") # Storing as an rds file locally to avoid
daph_post_text
View(daph_post_text)
monologue <- daph_post_text %>%
arrange(desc(duration)) %>%
top_n(10, duration) %>%
select(speaker, text, title, duration)
monologue
monologue <- daph_post_text %>%
filter(speaker != "Unknown") %>%
arrange(desc(duration)) %>%
top_n(10, duration) %>%
select(speaker, text, title, duration)
monologue
# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words
get_post_content <- function(ep_df) {
# Quick line for debugging purposes
# print(paste("Running for :", ep_df$url))
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
# Traverse down to the paragraph elements within the 'post' class
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
# Get the post title
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
# Rename the post_ps variable as it's automatically called '.' when converted from a list, which is problematic.
names(post_ps) <- "text"
# Filter to only elements which contain a timecode
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)|(Michael:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)|(Moe:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)|(Tim:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = as.numeric(ms(str_extract(text, "\\d{2}:\\d{2}"))),
time_end = lead(time_start, 1),
duration = time_end - time_start
)
post_ps
} else {
NULL
}
}
daph_post_text <- map_df(full_urls_split, get_post_content) %>%
arrange(episode, time_start)
saveRDS(daph_post_text, "datafiles/daph_post_text.rds") # Storing as an rds file locally to avoid
daph_post_text
# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words
get_post_content <- function(ep_df) {
# Quick line for debugging purposes
# print(paste("Running for :", ep_df$url))
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
# Traverse down to the paragraph elements within the 'post' class
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
# Get the post title
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
# Rename the post_ps variable as it's automatically called '.' when converted from a list, which is problematic.
names(post_ps) <- "text"
# Filter to only elements which contain a timecode
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)|(Michael:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)|(Moe:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)|(Tim:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
# Get the timecodes using regex extraction and handle variable timecode structure
time_start = case_when(
str_detect(text, "\\d:\\d{2}:\\d{2}") ~ as.numeric(hms(str_extract(text, "\\d:\\d{2}:\\d{2}"))),
TRUE ~ as.numeric(ms(str_extract(text, "\\d{2}:\\d{2}")))
),
time_end = lead(time_start, 1),
duration = time_end - time_start
)
post_ps
} else {
NULL
}
}
daph_post_text <- map_df(full_urls_split, get_post_content) %>%
arrange(episode, time_start)
View(daph_post_text)
saveRDS(daph_post_text, "datafiles/daph_post_text.rds") # Storing as an rds file locally to avoid
daph_post_text
daph_post_text$text[1]
monologue <- daph_post_text %>%
filter(speaker != "Unknown") %>%
arrange(desc(duration)) %>%
top_n(10, duration) %>%
select(speaker, text, title, duration)
monologue
monologue$text[1]
data("stop_words")
daph_post_words <- daph_post_text %>%
unnest_tokens(output = word, input = text, format = "text") %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "\\d{2}"))
saveRDS(daph_post_words, "datafiles/daph_post_words.rds")
data("stop_words")
daph_post_words <- daph_post_text %>%
unnest_tokens(output = word, input = text, format = "text") %>%
anti_join(stop_words)
saveRDS(daph_post_words, "datafiles/daph_post_words.rds")
swears <- daph_post_words %>%
filter(str_detect(word, "(shit)|(fuck)|(crap)"))
swears_by_speaker <- swears %>%
group_by(episode, speaker) %>%
filter(speaker!= "Unknown") %>%
summarise(ep_count = n()) %>%
ungroup() %>%
group_by(speaker) %>%
summarise(avg_swears = mean(ep_count, na.rm = TRUE)) %>%
mutate(speaker = fct_reorder(speaker, avg_swears))
swears_by_speaker %>%
ggplot(aes(x = speaker, y = avg_swears)) +
geom_col(fill = "#3FA0D9") +
theme_hc() +
labs(
x = "Speaker",
y = "Avg. swear jar deposits per episode",
title = "Avg. swear count by speaker",
subtitle = "Was there ever any doubt?")
# word count per episode by presenter
words_by_speaker <- daph_post_words %>%
group_by(episode, speaker) %>%
filter(speaker!= "Unknown") %>%
summarise(ep_count = n()) %>%
ungroup()
wbs_overall <- words_by_speaker %>%
group_by(speaker) %>%
summarise(avg_words = mean(ep_count, na.rm = TRUE)) %>%
mutate(speaker = fct_reorder(speaker, avg_words))
wbs_overall %>%
ggplot(aes(x = speaker, y = avg_words)) +
geom_col(fill = "#3FA0D9") +
theme_hc() +
xlab('Speaker') +
ylab('Avg words spoken per episode') +
ggtitle("Chatty Catty")
wbs_be <- words_by_speaker %>%
filter(ep_count!=0) %>%
ggplot(aes(as.numeric(episode), ep_count, colour = speaker)) +
geom_line(na.rm = TRUE) +
geom_point() +
labs(
title = "Words spoken by episode, by speaker",
y = "Words spoken",
x = "Episode number",
colour = "Speaker Name"
)
library(plotly)
ggplotly(wbs_be)
daph_sentiment <- daph_post_words %>%
inner_join(get_sentiments("bing"))
sentiment_by_episode <- daph_sentiment %>%
count(episode, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
senti_plot <- sentiment_by_episode %>%
filter(episode > 50) %>%
ggplot(aes(episode, sentiment)) +
geom_col()
ggplotly(senti_plot)
daph_sentiment <- daph_post_words %>%
inner_join(get_sentiments("bing"))
sentiment_by_episode <- daph_sentiment %>%
count(episode, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
senti_plot <- sentiment_by_episode %>%
filter(episode > 50) %>%
ggplot(aes(episode, sentiment)) +
geom_col() +
labs(
title = "DAPH overall sentiment by episode, 2017",
y = "Sentiment (positive - negative)",
x = "Episode Number"
)
ggplotly(senti_plot)
daph_sentiment %>%
filter(episode == 69) %>%
group_by(word, sentiment) %>%
count() %>%
arrange(sentiment, desc(n))
daph_sentiment %>%
filter(episode == 69, !str_detect(word, "bias")) %>%
group_by(sentiment) %>%
count()
daph_sentiment %>%
filter(episode == 57) %>%
group_by(word, sentiment) %>%
count() %>%
arrange(desc(sentiment), desc(n))
write_csv(daph_post_text, "output/daph_post_text.csv")
write_csv(daph_post_words, "output/daph_post_words.csv")
